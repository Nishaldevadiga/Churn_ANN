# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uAjXcsV58VNpD66NLc8MhgmuZqlXczhf
"""

!pip install tensorflow

import tensorflow as tf
print(tf.__version__)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv('sample_data/Churn_Modelling.csv')

data.head(5)

## divide into ind and dep features

x=data.iloc[:,3:13]
y=data.iloc[:,13]

x.head()

y.head()

x['Gender'].value_counts()

##feature engineering
geography=pd.get_dummies(x['Geography'],drop_first=True)

gender=pd.get_dummies(x['Gender'],drop_first=True)

geography = geography.astype(int)

gender=gender.astype(int)

geography

gender

#concatenate these variables to dataframe

x.drop(['Geography','Gender'],axis=1,inplace=True)

x.head()

x=pd.concat([x,geography,gender],axis=1)

x.head()

#splitting the data
from sklearn.model_selection import train_test_split


x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

#feature scaling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

x_train

x_test

x_train.shape

x_test.shape

#creating ANN
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU,PReLU,ELU,ReLU
from tensorflow.keras.layers import Dropout

##lets initialize the ANN
classifier=Sequential()

#Adding the input layer
classifier.add(Dense(units=11,activation='relu'))# activation function gets applied to next layer

#first hidden layer
classifier.add(Dense(units=7,activation='relu'))
#classifier.add(Dropout(0.3))

#second hidden layer
classifier.add(Dense(units=7,activation='relu'))

#add output layer
classifier.add(Dense(units=1,activation='sigmoid'))

classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])# adam uses by default lr

#Early stopping
import tensorflow as tf
early_stop=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.0001,
    patience=20,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0,
)

model_history=classifier.fit(x_train,y_train,validation_split=0.33,batch_size=10,epochs=1000,callbacks=early_stop)

model_history.history.keys()

# prompt: plot summary for accuracy

plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

y_pred=classifier.predict(x_test)
y_pred=(y_pred>=0.5)

y_pred

#confusion matrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)

cm

##accuracy
from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,y_test)

score

#get the weights
classifier.get_weights()

